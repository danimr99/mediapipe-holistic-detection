{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__fl94W7_BgQ"
   },
   "source": [
    "# Import and install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySy3dI8v-7VF",
    "outputId": "fa796671-cbc3-4f7b-80b3-55c3ec6e6915"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow tensorflow-gpu opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7W1484TNGd1E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[63424]: Class CaptureDelegate is implemented in both /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x137476480) and /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x13dfc8860). One of the two will be used. Which one is undefined.\n",
      "objc[63424]: Class CVWindow is implemented in both /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x1374764d0) and /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x117794a68). One of the two will be used. Which one is undefined.\n",
      "objc[63424]: Class CVView is implemented in both /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x1374764f8) and /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x117794a90). One of the two will be used. Which one is undefined.\n",
      "objc[63424]: Class CVSlider is implemented in both /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x137476520) and /Users/damura/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x117794ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE7AUTDpGqm_"
   },
   "source": [
    "# Extract landmarks using MediaPipe Holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcXNtSw_G-a4"
   },
   "source": [
    "Get MediaPipe Holistic model to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cjP1RjgXHPUs"
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnwR3Uv4HVMM"
   },
   "source": [
    "Get MediaPipe drawing utilities for the landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3QZVnaXCHUR9"
   },
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDRnBzQUHqkC"
   },
   "source": [
    "We define a function that, given an image and a MediaPipe model, is going to process the image and detect all available landmarks. \n",
    "\n",
    "Check the following link to discover all the MediaPipe models: https://google.github.io/mediapipe/solutions/solutions.html\n",
    "\n",
    "For a landmark to be detected, it must appear on the image. For example, if in the image only appears from our waist up, it will never detect the landmark of our feet.\n",
    "\n",
    "By default, OpenCV reads image on the channel format of BGR. In order to use MediaPipe, we need to convert it to RGB channel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1YAAA4UrIHzG"
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "\n",
    "  # Converting color from BGR to RGB\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # To improve performance, set image as no longer writeable\n",
    "  image.flags.writeable = False\n",
    "\n",
    "  # Get detection results\n",
    "  results = model.process(image)\n",
    "\n",
    "  # Set image writeable again\n",
    "  image.flags.writeable = True\n",
    "\n",
    "  # Convert color to original (from RGB to BGR)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "  return image, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kgidfu6SL20y"
   },
   "source": [
    "We define a function that will be responsible of drawing all the detected landmarks on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "THIU7wszMU12"
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "            \n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(255,87,51), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(77,255,51), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255,87,51), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(77,255,51), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "                              \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,87,51), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(77,255,51), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "                              \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,87,51), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(77,255,51), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9ONi9qpP5DP"
   },
   "source": [
    "Set up minimum detection confidence for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ioVS-8o8QCYc"
   },
   "outputs": [],
   "source": [
    "min_detection = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvO5OJdCQPr7"
   },
   "source": [
    "Set up minimum tracking confidence for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rim_czQxQY7-"
   },
   "outputs": [],
   "source": [
    "min_tracking = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sab9bvveOn_v"
   },
   "source": [
    "In order to test the MediaPipe model and extract landmarks, we are going to use the webcam to capture video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkj3jgwmPgqQ"
   },
   "source": [
    "Set up MediaPipe Holistic model and start to detect for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E7gcZozbQ1iY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=min_detection, min_tracking_confidence=min_tracking) as holistic:\n",
    "  while camera.isOpened():\n",
    "\n",
    "    # Read frame from the camera\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # Make detections using MediaPipe Holistic\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "    # Draw landmarks\n",
    "    draw_landmarks(image, results)\n",
    "                   \n",
    "    # Show frame with landmarks to screen on a window\n",
    "    cv2.imshow(\"Real-time detection\",image)\n",
    "\n",
    "    # Check if key 'q' has been pressed to stop recording\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "      break\n",
    "    \n",
    "  # Stop using camera\n",
    "  camera.release()\n",
    "\n",
    "  # Destroy window\n",
    "  cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
